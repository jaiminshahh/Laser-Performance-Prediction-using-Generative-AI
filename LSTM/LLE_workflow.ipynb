{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeF5HecNhgdp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dx_dy = 0.0333958286584664\n",
        "dt = 0.025"
      ],
      "metadata": {
        "id": "b0HB2zR411sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to scale the pulse data\n",
        "def scale_pulse(image, pulse, dx_dy, dt):\n",
        "    image_energy = image.sum() * dx_dy\n",
        "    pulse_energy = pulse.sum() * dt\n",
        "    K = image_energy / pulse_energy\n",
        "    scaled_pulse = K * pulse\n",
        "    return scaled_pulse\n",
        "\n",
        "\n",
        "def load_data(input_dir, output_dir):\n",
        "    # Get sorted lists of file names\n",
        "    input_files = sorted([f for f in os.listdir(input_dir) if f.endswith('.csv')])\n",
        "    output_files = sorted([f for f in os.listdir(output_dir) if f.endswith('.csv')])\n",
        "\n",
        "    # Print mappings of input files to output files\n",
        "    for input_file, output_file in zip(input_files, output_files):\n",
        "        print(f\"Mapping: {input_file} -> {output_file}\")  # Print the mapping\n",
        "\n",
        "        # Load input and output files without scaling\n",
        "        input_data = pd.read_csv(os.path.join(input_dir, input_file), header=None).values.flatten()\n",
        "        output_data = pd.read_csv(os.path.join(output_dir, output_file), header=None).values.flatten()\n",
        "\n",
        "        input_train_data_list.append(input_data)\n",
        "        output_train_data_list.append(output_data)\n",
        "\n",
        "    return np.array(input_train_data_list), np.array(output_train_data_list)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i-GAzzYfhwnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dx_dy = 0.0333958286584664\n",
        "dt = 0.025\n",
        "\n",
        "i_range = [i for i in range(20, 54)]\n",
        "j_range = range(1, 41)\n",
        "\n",
        "base_directory = '/content/drive/MyDrive/Data_LLE/Input_Pulse'\n",
        "scaled_pulse_base_directory = '/content/drive/MyDrive/Scaled_Input_Pulse'\n",
        "image_base_directory = '/content/drive/MyDrive/Data_LLE/Refined Data Set'\n",
        "\n",
        "for i in i_range:\n",
        "    for j in j_range:\n",
        "\n",
        "        input_pulse_file = os.path.join(base_directory, f'Run_{i}_BL4_s39708_InjPulsePower_InjEnergyFactor_{j}.csv')\n",
        "        scaled_pulse_base = os.path.join(scaled_pulse_base_directory, f'Run_{i}_BL4_s39708_InjPulsePower_InjEnergyFactor_{j}.csv')\n",
        "        input_image_file = os.path.join(image_base_directory, f'Run {i} - BL4, s39708', f'Run_{i}_BL4_s39708_Inj_256x256_InjEnergyFactor_{j}.csv')\n",
        "\n",
        "\n",
        "        if os.path.isfile(input_pulse_file) and os.path.isfile(input_image_file):\n",
        "\n",
        "            image_data = pd.read_csv(input_image_file, header=None).values\n",
        "\n",
        "\n",
        "            input_pulse = pd.read_csv(input_pulse_file, header=None).iloc[0].values\n",
        "            scaled_input_pulse = scale_pulse(image_data, input_pulse)\n",
        "            scaled_input_pulse = scaled_input_pulse.transpose()\n",
        "\n",
        "            # Convert to DataFrame and save to CSV\n",
        "            pd.DataFrame(scaled_input_pulse).to_csv(scaled_pulse_base, header=None, index=False)\n",
        "\n",
        "        else:\n",
        "            print(f\"Files not found for i={i}, j={j}: Skipping.\")\n",
        "\n",
        "print(\"Data loading complete.\")"
      ],
      "metadata": {
        "id": "0wNjWvLWhoyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "output_train_dir = '/content/drive/MyDrive/Scaled_Input_Pulse'\n",
        "output_test_dir = '/content/drive/MyDrive/Scaled_Test/Output'\n",
        "i_range = [22, 26, 30, 34, 48]\n",
        "j_range = range(1, 41)\n",
        "# Move files\n",
        "for i in i_range:\"\"\n",
        "    for j in j_range:\n",
        "        # Construct the file name\n",
        "        file_name = f'Run_{i}_BL4_s39708_UVDBS_Power_InjEnergyFactor_{j}.csv'\n",
        "        source_path = os.path.join(output_train_dir, file_name)\n",
        "        destination_path = os.path.join(output_test_dir, file_name)\n",
        "\n",
        "        # Check if the file exists before moving\n",
        "        if os.path.exists(source_path):\n",
        "            shutil.move(source_path, destination_path)\n",
        "            print(f\"Moved: {source_path} -> {destination_path}\")\n",
        "        else:\n",
        "            print(f\"File not found: {source_path}\")"
      ],
      "metadata": {
        "id": "i3fmaMI84-OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to hold each loaded sample\n",
        "input_train_data_list = []\n",
        "output_train_data_list = []\n",
        "\n",
        "def load_data(input_dir, output_dir):\n",
        "    # Get sorted lists of file names\n",
        "    input_files = sorted([f for f in os.listdir(input_dir) if f.endswith('.csv')])\n",
        "    output_files = sorted([f for f in os.listdir(output_dir) if f.endswith('.csv')])\n",
        "\n",
        "    for input_file, output_file in zip(input_files, output_files):\n",
        "        # Load input and output files without scaling\n",
        "        input_data = pd.read_csv(os.path.join(input_dir, input_file), header=None).iloc[:, 59:59+576].values.flatten()\n",
        "        output_data = pd.read_csv(os.path.join(output_dir, output_file), header=None).iloc[:, 59:59+576].values.flatten()\n",
        "        print(f\"{input_file}: {input_data.shape}\")\n",
        "        print(f\"{output_file}: {output_data.shape}\")\n",
        "        input_train_data_list.append(input_data)\n",
        "        output_train_data_list.append(output_data)\n",
        "\n",
        "    return np.array(input_train_data_list), np.array(output_train_data_list)\n",
        "\n",
        "# Specify directories\n",
        "input_train_dir = '/content/drive/MyDrive/Scaled_Input_Pulse'\n",
        "output_train_dir = '/content/drive/MyDrive/Data_LLE/Output_Pulse'\n",
        "\n",
        "# Load the data\n",
        "X_train, y_train = load_data(input_train_dir, output_train_dir)\n",
        "\n",
        "# Reshape X and y for LSTM compatibility (samples, time steps, features)\n",
        "X_train = X_train.reshape(len(X_train), 576, 1)\n",
        "y_train = y_train.reshape(len(y_train), 576, 1)\n",
        "\n",
        "print(f\"Shape of X (input): {X_train.shape}\")\n",
        "print(f\"Shape of y (output): {y_train.shape}\")"
      ],
      "metadata": {
        "id": "VdhQwDeiiC64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to hold each loaded sample\n",
        "input_test_data_list = []\n",
        "output_test_data_list = []\n",
        "\n",
        "def load_data(input_dir, output_dir):\n",
        "    # Get sorted lists of file names\n",
        "    input_files = sorted([f for f in os.listdir(input_dir) if f.endswith('.csv')])\n",
        "    output_files = sorted([f for f in os.listdir(output_dir) if f.endswith('.csv')])\n",
        "\n",
        "    for input_file, output_file in zip(input_files, output_files):\n",
        "        # Load input and output files without scaling\n",
        "        input_data = pd.read_csv(os.path.join(input_dir, input_file), header=None).iloc[:, 59:59+576].values.flatten()\n",
        "        output_data = pd.read_csv(os.path.join(output_dir, output_file), header=None).iloc[:, 59:59+576].values.flatten()\n",
        "\n",
        "        input_test_data_list.append(input_data)\n",
        "        output_test_data_list.append(output_data)\n",
        "\n",
        "    return np.array(input_test_data_list), np.array(output_test_data_list)\n",
        "\n",
        "# Specify directories\n",
        "input_test_dir = '/content/drive/MyDrive/Scaled_Test/Input'\n",
        "output_test_dir = '/content/drive/MyDrive/Scaled_Test/Output'\n",
        "\n",
        "# Load the data\n",
        "X_test, y_test = load_data(input_test_dir, output_test_dir)\n",
        "\n",
        "# Reshape X and y for LSTM compatibility (samples, time steps, features)\n",
        "X_test = X_test.reshape(len(X_test), 576, 1)\n",
        "y_test = y_test.reshape(len(y_test), 576, 1)\n",
        "\n",
        "print(f\"Shape of X (input): {X_test.shape}\")\n",
        "print(f\"Shape of y (output): {y_test.shape}\")"
      ],
      "metadata": {
        "id": "ozjXLig-2zRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_scaler = MinMaxScaler()\n",
        "output_scaler = MinMaxScaler()"
      ],
      "metadata": {
        "id": "olZ_cYfbHR9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape data to 2D for scaling, then reshape back\n",
        "X_train_reshaped = X_train.reshape(-1, 2)  # Combine samples and time steps for scaling\n",
        "X_train_normalized = input_scaler.fit_transform(X_train_reshaped).reshape(X_train.shape)  # Reshape back to original shape\n",
        "\n",
        "y_train_reshaped = y_train.reshape(-1, 1)  # Output is 1D per sample, so reshape to 2D for scaling\n",
        "y_train_normalized = output_scaler.fit_transform(y_train_reshaped).reshape(y_train.shape)"
      ],
      "metadata": {
        "id": "JuLLFJvCiJnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape data to 2D for scaling, then reshape back\n",
        "X_test_reshaped = X_test.reshape(-1, 2)  # Combine samples and time steps for scaling\n",
        "X_test_normalized = input_scaler.fit_transform(X_test_reshaped).reshape(X_test.shape)  # Reshape back to original shape\n",
        "\n",
        "y_test_reshaped = y_test.reshape(-1, 1)  # Output is 1D per sample, so reshape to 2D for scaling\n",
        "y_test_normalized = output_scaler.fit_transform(y_test_reshaped).reshape(y_test.shape)"
      ],
      "metadata": {
        "id": "SfQ_IXeGiKUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "X_train_train, X_valid, y_train_train, y_valid = train_test_split(X_train_normalized, y_train_normalized, test_size=0.15, random_state=42)"
      ],
      "metadata": {
        "id": "QUSaqrd48gId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# Custom correlation loss function\n",
        "def correlation_loss(y_true, y_pred):\n",
        "    # Calculate the mean of the true and predicted values\n",
        "    y_true_mean = tf.reduce_mean(y_true)\n",
        "    y_pred_mean = tf.reduce_mean(y_pred)\n",
        "\n",
        "    # Calculate the covariance between the true and predicted values\n",
        "    cov = tf.reduce_mean((y_true - y_true_mean) * (y_pred - y_pred_mean))\n",
        "\n",
        "    # Calculate the standard deviations of the true and predicted values\n",
        "    y_true_std = tf.math.reduce_std(y_true)\n",
        "    y_pred_std = tf.math.reduce_std(y_pred)\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    correlation = cov / (y_true_std * y_pred_std + 1e-6)  # Adding epsilon to avoid division by zero\n",
        "\n",
        "    # Return the negative correlation as the loss (since we want to maximize correlation)\n",
        "    return correlation\n",
        "\n",
        "def combined_loss(y_true, y_pred, alpha=0.5, beta=0.1):\n",
        "    # L1 loss (Mean Absolute Error)\n",
        "    l1_loss = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
        "\n",
        "    # Calculate the correlation coefficient between y_true and y_pred\n",
        "    corr_loss = correlation_loss(y_true, y_pred)\n",
        "\n",
        "    penalty = tf.where(corr_loss < 0, tf.abs(corr_loss), 0)\n",
        "\n",
        "    return alpha * l1_loss + (1 - alpha) * (1 - corr_loss) + beta * penalty\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HAFXkMPh-qqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "\n",
        "    # Second Bidirectional LSTM layer with reduced units and regularization\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.001))),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    # Third Bidirectional LSTM layer\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.001))),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    # Fourth Bidirectional LSTM layer with reduced units\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.001))),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "     # Five Bidirectional LSTM layer with reduced units\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.001))),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    # Final Bidirectional LSTM layer without return_sequences for the final output\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256)),\n",
        "\n",
        "    # Output Dense layer with tanh activation\n",
        "    tf.keras.layers.Dense(576, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.001))\n",
        "])\n"
      ],
      "metadata": {
        "id": "SCk-Epl_H5dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model with the combined loss function\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss=lambda y_true, y_pred: combined_loss(y_true, y_pred, alpha=0.5, beta=0.1))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "l37DPB_n-QPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks for learning rate reduction and early stopping\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)"
      ],
      "metadata": {
        "id": "JogEZLYD-SZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = model.fit(X_train_train, y_train_train,\n",
        "                    epochs=300,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    batch_size=16,\n",
        "                    callbacks=[reduce_lr, early_stopping])"
      ],
      "metadata": {
        "id": "aiaShzYm-U9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final model to a safe location\n",
        "final_model_path = '/content/drive/MyDrive/Scaled_Test/final_best_model.h5'\n",
        "model.save(final_model_path)\n",
        "print(f\"Best model saved to: {final_model_path}\")"
      ],
      "metadata": {
        "id": "IkgLh-Np9huC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training and validation loss over epochs\n",
        "def plot_loss(history):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss (MSE)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss(history)\n"
      ],
      "metadata": {
        "id": "8r5eyFSvE7f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_test_data_list = []\n",
        "input_file_names = []  # To store file names for saving predictions\n",
        "\n",
        "def load_test_data_with_filenames(input_dir):\n",
        "    input_files = sorted([f for f in os.listdir(input_dir) if f.endswith('.csv')])\n",
        "\n",
        "    for input_file in input_files:\n",
        "        input_data = pd.read_csv(os.path.join(input_dir, input_file), header=None).iloc[:, 59:59+576].values.flatten()\n",
        "        input_test_data_list.append(input_data)\n",
        "        input_file_names.append(input_file)  # Save file names for later use\n",
        "\n",
        "    return np.array(input_test_data_list), input_file_names\n",
        "\n",
        "# Load test data\n",
        "X_test, input_file_names = load_test_data_with_filenames(input_test_dir)\n",
        "# Generate predictions\n",
        "predictions_normalized = model.predict(X_test_normalized)\n",
        "\n",
        "# Inverse transform predictions to original scale\n",
        "predictions = output_scaler.inverse_transform(predictions_normalized.reshape(-1, 2)).reshape(predictions_normalized.shape)\n",
        "\n",
        "# Save predictions with the same filenames\n",
        "output_predictions_dir = '/content/drive/MyDrive/Scaled_Test/Predictions'\n",
        "\n",
        "if not os.path.exists(output_predictions_dir):\n",
        "    os.makedirs(output_predictions_dir)\n",
        "\n",
        "for i, prediction in enumerate(predictions):\n",
        "    output_file_path = os.path.join(output_predictions_dir, input_file_names[i])\n",
        "    prediction_flat = prediction.flatten()  # Flatten prediction for saving\n",
        "    pd.DataFrame(prediction_flat).to_csv(output_file_path, index=False, header=False)\n",
        "\n",
        "print(f\"Predictions saved in directory: {output_predictions_dir}\")"
      ],
      "metadata": {
        "id": "5yy3T9RvE0BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [22, 26, 30, 34, 48]:\n",
        "    for j in range(1, 41):\n",
        "      file_path = f'/content/drive/MyDrive/Scaled_Test/Combined/predicted_Run_{i}_BL4_s39708_UVDBS_Power_InjEnergyFactor_{j}.csv'\n",
        "      data = pd.read_csv(file_path)\n",
        "\n",
        "# Extract Prediction and Actual columns\n",
        "      predictions = data['Prediction']\n",
        "      actuals = data['Actual']\n",
        "\n",
        "# Plot predictions and actuals\n",
        "      plt.figure(figsize=(50, 30))\n",
        "      plt.plot(predictions, label='Predictions', linestyle='-', marker='.')\n",
        "      plt.plot(actuals, label='Actuals', linestyle='-', marker='.')\n",
        "      plt.title(f'predicted_Run_{i}_BL4_s39708_UVDBS_Power_InjEnergyFactor_{j}')\n",
        "      plt.xlabel('Index')\n",
        "      plt.ylabel('Values (Normalized)')\n",
        "      plt.legend()\n",
        "      plt.grid(True)\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(f'/content/drive/MyDrive/Scaled_Test/Plots/predicted_Run_{i}_BL4_s39708_UVDBS_Power_InjEnergyFactor_{j}.png')\n",
        "      plt.close()"
      ],
      "metadata": {
        "id": "P7tpQI9YHp6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import os\n",
        "\n",
        "# Function to calculate Mean Squared Error (MSE)\n",
        "def calculate_mse(predictions, actuals):\n",
        "    \"\"\"\n",
        "    Calculates the Mean Squared Error (MSE) between predicted and actual values.\n",
        "    \"\"\"\n",
        "    return mean_squared_error(actuals, predictions)\n",
        "\n",
        "# Initialize lists to store results\n",
        "results = []\n",
        "\n",
        "# Min-Max scaler for normalization\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Output directory for combined files\n",
        "combined_dir = '/content/drive/MyDrive/Scaled_Test/Combined'\n",
        "os.makedirs(combined_dir, exist_ok=True)\n",
        "\n",
        "# Iterate through each sample\n",
        "for run in [22, 26, 30, 34, 48]:\n",
        "    for j in range(1, 41):\n",
        "        # Construct file paths\n",
        "        pred_path = f'/content/drive/MyDrive/Scaled_Test/Predictions/Run_{run}_BL4_s39708_InjPulsePower_InjEnergyFactor_{j}.csv'\n",
        "        actual_path = f'/content/drive/MyDrive/Scaled_Test/Output/Run_{run}_BL4_s39708_UVDBS_Power_InjEnergyFactor_{j}.csv'\n",
        "\n",
        "        # Read the prediction and actual CSVs\n",
        "        df_pred = pd.read_csv(pred_path, header=None).values.flatten()\n",
        "        df_actual = pd.read_csv(actual_path, header=None).iloc[:, 59:59+576].values.flatten()\n",
        "\n",
        "        # Create DataFrames for predicted and actual values\n",
        "        df_preds = pd.DataFrame(df_pred, columns=['Prediction'])\n",
        "        df_actuals = pd.DataFrame(df_actual, columns=['Actual'])\n",
        "\n",
        "        # Calculate the total energy for predicted and actual values\n",
        "        predicted_energy = df_preds['Prediction'].sum() * 0.025  # Sum and scale predictions\n",
        "        ground_truth_energy = df_actuals['Actual'].sum() * 0.025  # Sum and scale normalized actuals\n",
        "\n",
        "        # Calculate the energy difference\n",
        "        energy_difference = abs(predicted_energy - ground_truth_energy)\n",
        "\n",
        "        # Calculate MSE for the current prediction and actual values\n",
        "        mse = calculate_mse(df_preds['Prediction'], df_actuals['Actual'])\n",
        "\n",
        "        # Append the result for this file to the results list\n",
        "        results.append({\n",
        "            \"Run\": run,\n",
        "            \"File\": j,\n",
        "            \"MSE\": mse,\n",
        "            \"Predicted Energy\": predicted_energy,\n",
        "            \"Ground Truth Energy\": ground_truth_energy,\n",
        "            \"Energy Difference\": energy_difference\n",
        "        })\n",
        "\n",
        "        # Combine predicted and normalized actual values into one DataFrame\n",
        "        combined_df = pd.concat([df_preds, df_actuals], axis=1)\n",
        "\n",
        "        # Save the combined DataFrame to a new CSV\n",
        "        combined_file_path = f'/content/drive/MyDrive/Scaled_Test/Combined/predicted_Run_{run}_BL4_s39708_UVDBS_Power_InjEnergyFactor_{j}.csv'\n",
        "        combined_df.to_csv(combined_file_path, index=False)\n",
        "\n",
        "        print(f\"Saved combined file: {combined_file_path}\")\n",
        "\n",
        "# Convert the results list to a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "results_path = '/content/drive/MyDrive/Scaled_Test/energy_comparison_results.csv'\n",
        "results_df.to_csv(results_path, index=False)\n",
        "\n",
        "print(f\"Energy comparison results saved to: {results_path}\")\n"
      ],
      "metadata": {
        "id": "uFzwjJ1hVcnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df['Energy Difference'].mean()"
      ],
      "metadata": {
        "id": "wiFkdy6NgNGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc  # For manual garbage collection\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "for i in [22, 26, 30, 34, 48]:\n",
        "    for j in range(1, 41):\n",
        "        file_path = f'/content/drive/MyDrive/Scaled_Test/Combined/predicted_Run_{i}_BL4_s39708_UVDBS_Power_InjEnergyFactor_{j}.csv'\n",
        "\n",
        "        # Read only the necessary columns to save memory\n",
        "        data = pd.read_csv(file_path, usecols=['Prediction', 'Actual'])\n",
        "\n",
        "        # Extract Prediction and Actual columns\n",
        "        predictions = data['Prediction']\n",
        "        actuals = data['Actual']\n",
        "\n",
        "        # Plot predictions and actuals\n",
        "        plt.figure(figsize=(20, 12))  # Reduce figure size to conserve memory\n",
        "        plt.plot(predictions, label='Predictions', linestyle='-', marker='.', markersize=2)\n",
        "        plt.plot(actuals, label='Actuals', linestyle='-', marker='.', markersize=2)\n",
        "        plt.title(f'predicted_Run_{i}_BL4_s39708_UVDBS_Power_InjEnergyFactor_{j}')\n",
        "        plt.xlabel('Index')\n",
        "        plt.ylabel('Values (Normalized)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the plot\n",
        "        plot_path = f'/content/drive/MyDrive/Scaled_Test/Plots/predicted_Run_{i}_BL4_s39708_UVDBS_Power_InjEnergyFactor_{j}.png'\n",
        "        plt.savefig(plot_path, dpi=300)  # Use higher DPI for better resolution\n",
        "        plt.close()\n",
        "\n",
        "        # Clear memory\n",
        "        del data, predictions, actuals\n",
        "        gc.collect()\n",
        "\n",
        "print(\"All plots generated and saved successfully.\")"
      ],
      "metadata": {
        "id": "ucHy5_2GKaEH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}